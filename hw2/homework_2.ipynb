{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da4b91a-9bd7-43e9-9bde-6bb91bf118c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6db5f-a1de-49bc-85b4-f43a99d6b109",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccef35f-0704-42e6-a22d-7f31e6293fdb",
   "metadata": {},
   "source": [
    "## 1. Image Stitching "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526f151-c409-489d-aaf1-aa4588af6d43",
   "metadata": {},
   "source": [
    "This problem will walk you through the process of image stitching by detecting and matching corners, and estimating the homography based the matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24306eb-4260-4cc0-8818-d82a59c229dc",
   "metadata": {},
   "source": [
    "### 1a. Detect Keypoints and Calculate Descriptors (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9850a0b-c2e2-4f1f-9cd6-8a386111ff33",
   "metadata": {},
   "source": [
    "Load the sample image pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb7afe-c7f9-4b82-9f10-530804d6cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('data/graff1.png')\n",
    "img2 = cv2.imread('data/graff2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f4a3c-d7ac-46e5-bafa-8bbfc3ebb89f",
   "metadata": {},
   "source": [
    "Since we have implemented the Harris corner detector from scratch in the weekly notebooks, in this problem we will use OpenCV to detect corners and get their descriptors. In the cell below, detect feature points and calculate their descriptors in both images using OpenCV's ORB detector and descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437db4d-1030-45f1-8fea-89a752e75783",
   "metadata": {},
   "outputs": [],
   "source": [
    "kps1, des1 = # TODO\n",
    "kps2, des2 = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b58bb-7360-430f-8400-b9eec8cc0a1e",
   "metadata": {},
   "source": [
    "Visualize locations of detected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a285e2c-1ce1-4f04-ae10-66dbbd970457",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_img1 = cv2.drawKeypoints(img1, kps1, None, color=(0,255,0))\n",
    "kp_img2 = cv2.drawKeypoints(img2, kps2, None, color=(0,255,0))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(cv2.cvtColor(kp_img1, cv2.COLOR_BGR2RGB))\n",
    "axs[1].imshow(cv2.cvtColor(kp_img2, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1378c60-0054-49fe-b572-4ac1cc1b0d73",
   "metadata": {},
   "source": [
    "### 1b. Match Keypoints (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ce33c-1a73-4ea1-b664-bf0a627de898",
   "metadata": {},
   "source": [
    "Check out OpenCV's online docs and answer the following question:\n",
    "- What attributes does each detected keypoint have? Briefly explain what they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4930f7-0ad8-403b-8c0a-57829c5d0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your answer below:\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79102f1d-43f2-411e-aa82-54b6555df32b",
   "metadata": {},
   "source": [
    "- What is the \"apparent\" size of each descriptor and what data type does it have? (You can easily find the answer by printing the `shape` and `dtype` of a descriptor.) The ORB [paper](https://ieeexplore.ieee.org/document/6126544) says \"*we propose a very fast **binary** descriptor based on BRIEF, called ORB*.\" Based on this, what is the actual length of each binary descriptor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29b322-13a6-4160-b021-d6a97988b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your answer below:\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622563ab-65cf-4027-85e1-7eb0994e4d1b",
   "metadata": {},
   "source": [
    "- Given two ORB descriptors, how should you measure their distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b338494-c1c5-43b1-9468-d9a0a1a6bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your answer below:\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13998e96-60ab-4cbc-9f72-fa1acdf214ed",
   "metadata": {},
   "source": [
    "Using `cv2.BFMatcher`, calculate the matching between the two lists of keypoints. Be sure to use the correct distance measurement. Also, set `crossCheck=True` when creating the matcher. After you get the matches, sort them by distance and call `cv2.drawMatches()` to display the 100 closest matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb59c8-c7a9-48eb-8551-b7fc72e4f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = # TODO\n",
    "match_img = cv2.drawMatches(img1, kps1, img2, kps2, matches[:100], None,\n",
    "    matchColor=(0,255,0), singlePointColor=(0,255,0))\n",
    "plt.imshow(cv2.cvtColor(match_img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4334912c-037a-4a89-8d5b-1e9ba9311404",
   "metadata": {},
   "source": [
    "### 1c. Calculate Homography Between Matching Keypoints (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722678c5-3ca4-4c25-b7fc-371918b45bbb",
   "metadata": {},
   "source": [
    "Implement the following function to calculate the projective transformation that maps a list of (x, y) coordinates `pts1` to the list of matching coordinates `pts2`. The return value is a 3x3 array representing the transformation matrix. You don't need to worry about there being outlier matches between `pts1` and `pts2` here.\n",
    "\n",
    "You should implement this from from scratch and may **NOT** simply call functions from OpenCV or other libraries to solve this for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68573ef-3f90-4999-9b97-c99d7e2a7ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_homography(pts1, pts2):\n",
    "    \"\"\"Estimate the affine transformation between two lists of matching\n",
    "    keypoint coordinates.\n",
    "    \n",
    "    Args:\n",
    "    - pts1: List of (x, y) coordinates.\n",
    "    - pts2: List of (x, y) coordinates.\n",
    "    \n",
    "    Returns:\n",
    "    - t: 3x3 array representing the projective transformation matrix that\n",
    "          maps `pts1` to `pts2`.\n",
    "    \"\"\"\n",
    "    assert len(pts1) == len(pts2), \\\n",
    "        \"`pts1` and `pts2` must have equal length.\"\n",
    "    # TODO\n",
    "    return t\n",
    "\n",
    "\n",
    "# Test the function on sample keypoints.\n",
    "pts1 = [\n",
    "    [313.9, 316.8],\n",
    "    [120.0, 284.0],\n",
    "    [437.2, 501.1],\n",
    "    [221.0, 510.0],\n",
    "]\n",
    "pts2 = [\n",
    "    [318.0, 372.0],\n",
    "    [151.0, 390.0],\n",
    "    [466.6, 504.0],\n",
    "    [304.0, 571.0],\n",
    "]\n",
    "t = get_homography(pts1, pts2)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d45092-1d33-4999-894a-e4a93833b6bb",
   "metadata": {},
   "source": [
    "### 1d. Estimate Affine Transformation Between Images Using RANSAC (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f44f6-eb9f-4749-9754-10136ad26ce0",
   "metadata": {},
   "source": [
    "Implement the following function to estimate the homography between two images given keypoints and matches. There may be outliers in the keypoint matches and you should implement the RANSAC algorithm to reject them. You should implement RANSAC from scratch and may **NOT** simply call the corresponding functions in OpenCV or any other existing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54075177-0d73-4f0a-9278-f2fdf1d68d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def estimate_transformation_ransac(kps1, kps2, matches, transform_func,\n",
    "    n_samples, n_trials, inlier_thresh):\n",
    "    \"\"\"Estimate the transformation between two matching lists of keypoints \n",
    "    using RANSAC.\n",
    "    \n",
    "    Args:\n",
    "    - kps1: A list of `cv2.KeyPoint` objects.\n",
    "    - kps2: A list of `cv2.KeyPoint` objects.\n",
    "    - matches: A list of `cv2.DMatch` objects representing the matches\n",
    "          between `kps1` and `kps2`.\n",
    "    - transform_func: Function used to estimate the transformation given\n",
    "          matching points.\n",
    "    - n_samples: The number of samples in each RANSAC trial.\n",
    "    - n_trials: The total number of RANSAC trials.\n",
    "    - inlier_thresh: The threshold used to determine whether a match is an\n",
    "          inlier or not.\n",
    "    \n",
    "    Returns:\n",
    "    - transform: The 3x3 transformation matrix that maps kps1 to kps2.\n",
    "    - mask: A list of Boolean values representing whether the corresponding\n",
    "          match is an inlier or not.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return transform, mask\n",
    "\n",
    "transform, mask = # TODO\n",
    "print(f\"The transformation matrix:\\n{transform}.\")\n",
    "print(f\"{np.sum(mask)} out of {len(matches)} matches are inliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68715fce-9aec-4a18-9970-87fb791eb339",
   "metadata": {},
   "source": [
    "The following cell warps image 1 based on the estimated homography and displays the overlay of the aligned images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12195f5-5c8d-46c3-a73a-50bd8c39bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images(img1, img2, transform):\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "\n",
    "    # Warp corners of image 1 to coordinates system of image 2.\n",
    "    corners = np.array([\n",
    "        [0., w1, w1, 0.],\n",
    "        [0., 0., h1, h1],\n",
    "        [1., 1., 1., 1.]\n",
    "    ])\n",
    "    new_corners = transform @ corners\n",
    "    new_corners /= new_corners[2, :]\n",
    "    # Calculate range of warped image 1.\n",
    "    l = np.min(new_corners[0, :])\n",
    "    r = np.max(new_corners[0, :])\n",
    "    t = np.min(new_corners[1, :])\n",
    "    b = np.max(new_corners[1, :])\n",
    "    # Calculate size of the stitched image.\n",
    "    l = np.min((l,  0))\n",
    "    r = np.max((r, w2))\n",
    "    t = np.min((t,  0))\n",
    "    b = np.max((b, h2))\n",
    "    w = int(np.round(r-l))\n",
    "    h = int(np.round(b-t))\n",
    "    # Calculate how much we should shift the stitched image.\n",
    "    dx = max((-l, 0.))\n",
    "    dy = max((-h, 0.))\n",
    "    shift = np.array([\n",
    "        [1., 0., dx],\n",
    "        [0., 1., dy],\n",
    "        [0., 0., 1.]\n",
    "    ])\n",
    "\n",
    "    # Warp input images and overlay them.\n",
    "    warped_img1 = cv2.warpPerspective(img1, shift @ transform, (w, h))\n",
    "    warped_img2 = cv2.warpPerspective(img2, shift, (w, h))\n",
    "    overlayed_img = cv2.addWeighted(warped_img1, 0.5, warped_img2, 0.5, 0.0)\n",
    "    return overlayed_img\n",
    "\n",
    "\n",
    "stitched_img = stitch_images(img1, img2, transform)\n",
    "plt.imshow(cv2.cvtColor(stitched_img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f1503-b70d-492d-a8b0-78c8be927624",
   "metadata": {},
   "source": [
    "# 2. Disparity Map (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752cf9f8-473d-4134-89e7-df08a36f5987",
   "metadata": {},
   "source": [
    "In this problem you will find dense correspondences between the stereo image pair below and calculate the disparity map. This will be done by doing template matching between image windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f559de-db1e-4c3a-8948-f3f67e61d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('data/left.bmp')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGRA2GRAY) / 255.\n",
    "\n",
    "img2 = cv2.imread('data/right.bmp')\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGRA2GRAY) / 255.\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(img1, cmap='gray')\n",
    "ax1.set_title('left')\n",
    "ax2.imshow(img2, cmap='gray')\n",
    "ax2.set_title('right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f17f8d-1c68-4d69-a7bd-eab5e776dbda",
   "metadata": {},
   "source": [
    "In the cell below, implement a system to calculate the disparity map and visualize the results. You can assume that the camera angle of the images are already rectified, so the epipolar lines are all horizontal. Use SSD as the similarity measure between windows. Set the window size to `7` and the maximum disparity value to `40`.\n",
    "\n",
    "Recall from the lectures we covered a naive implementation and a more efficient one. Please implement the efficient version.\n",
    "\n",
    "In addition, operations on image arrays (such as shifting an image or comparing two image arrays) should be implemented in the vectorized form for more efficiency, instead of requiring two for-loops to loop through pixels one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983032b-1433-4728-9f9a-3f673855bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "\n",
    "\n",
    "assert img1.shape == img2.shape\n",
    "h, w = img1.shape\n",
    "\n",
    "dmin, dmax = 0, 40  # disparity search range\n",
    "s = 7  # windows size\n",
    "\n",
    "# TODO\n",
    "\n",
    "plt.imshow(disp, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
